{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fc574-27f4-4b46-a895-7a304c78deb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4642227-8c95-4058-84bd-75f14c324c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84ed8de4-91d0-44fc-93f7-cd260684c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: X=(155620, 100), y=(155620, 1)\n",
      "Min y value: 1, Max y value: 15413, Vocab Size: 15414\n",
      "Epoch 1/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 392ms/step - accuracy: 0.0466 - loss: 7.5059\n",
      "Epoch 2/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 384ms/step - accuracy: 0.0570 - loss: 6.8457\n",
      "Epoch 3/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 385ms/step - accuracy: 0.0569 - loss: 6.7673\n",
      "Epoch 4/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 384ms/step - accuracy: 0.0638 - loss: 6.7164\n",
      "Epoch 5/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 380ms/step - accuracy: 0.0603 - loss: 6.7110\n",
      "Epoch 6/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 371ms/step - accuracy: 0.0664 - loss: 6.6970\n",
      "Epoch 7/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 378ms/step - accuracy: 0.0731 - loss: 6.6240\n",
      "Epoch 8/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 382ms/step - accuracy: 0.0740 - loss: 6.6166\n",
      "Epoch 9/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 308ms/step - accuracy: 0.0757 - loss: 6.6308\n",
      "Epoch 10/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 294ms/step - accuracy: 0.0688 - loss: 6.6464\n",
      "Epoch 11/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 365ms/step - accuracy: 0.0748 - loss: 6.6000\n",
      "Epoch 12/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 369ms/step - accuracy: 0.0700 - loss: 6.6204\n",
      "Epoch 13/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - accuracy: 0.0753 - loss: 6.5356  \n",
      "Epoch 14/180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mr.Laptop point\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 354ms/step - accuracy: 0.0830 - loss: 6.3373\n",
      "Epoch 15/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 342ms/step - accuracy: 0.0783 - loss: 6.4015\n",
      "Epoch 16/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 356ms/step - accuracy: 0.0784 - loss: 6.4058\n",
      "Epoch 17/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 333ms/step - accuracy: 0.0833 - loss: 6.2576\n",
      "Epoch 18/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 328ms/step - accuracy: 0.0831 - loss: 6.3227\n",
      "Epoch 19/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 319ms/step - accuracy: 0.0794 - loss: 6.3958\n",
      "Epoch 20/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 356ms/step - accuracy: 0.0815 - loss: 6.4047\n",
      "Epoch 21/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 345ms/step - accuracy: 0.0826 - loss: 6.3522\n",
      "Epoch 22/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 345ms/step - accuracy: 0.0830 - loss: 6.4288\n",
      "Epoch 23/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 354ms/step - accuracy: 0.0887 - loss: 6.3998\n",
      "Epoch 24/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 354ms/step - accuracy: 0.0839 - loss: 6.3626\n",
      "Epoch 25/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 355ms/step - accuracy: 0.0819 - loss: 6.4613\n",
      "Epoch 26/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 54ms/step - accuracy: 0.0882 - loss: 6.4584  \n",
      "Epoch 27/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 341ms/step - accuracy: 0.0938 - loss: 6.2268\n",
      "Epoch 28/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 352ms/step - accuracy: 0.0890 - loss: 6.2536\n",
      "Epoch 29/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 343ms/step - accuracy: 0.0919 - loss: 6.2003\n",
      "Epoch 30/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 346ms/step - accuracy: 0.0882 - loss: 6.1872\n",
      "Epoch 31/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 347ms/step - accuracy: 0.0928 - loss: 6.1980\n",
      "Epoch 32/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 341ms/step - accuracy: 0.0956 - loss: 6.1531\n",
      "Epoch 33/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 352ms/step - accuracy: 0.0897 - loss: 6.2392\n",
      "Epoch 34/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 346ms/step - accuracy: 0.0883 - loss: 6.2097\n",
      "Epoch 35/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 349ms/step - accuracy: 0.1000 - loss: 6.1872\n",
      "Epoch 36/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 343ms/step - accuracy: 0.0983 - loss: 6.2003\n",
      "Epoch 37/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 339ms/step - accuracy: 0.0984 - loss: 6.1938\n",
      "Epoch 38/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 329ms/step - accuracy: 0.0922 - loss: 6.2139\n",
      "Epoch 39/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.0885 - loss: 6.2379  \n",
      "Epoch 40/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 348ms/step - accuracy: 0.0922 - loss: 6.0436\n",
      "Epoch 41/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 313ms/step - accuracy: 0.0943 - loss: 6.0719\n",
      "Epoch 42/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 338ms/step - accuracy: 0.0967 - loss: 6.1009\n",
      "Epoch 43/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 331ms/step - accuracy: 0.0953 - loss: 6.0520\n",
      "Epoch 44/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 320ms/step - accuracy: 0.0927 - loss: 6.1171\n",
      "Epoch 45/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 332ms/step - accuracy: 0.0946 - loss: 6.1328\n",
      "Epoch 46/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 339ms/step - accuracy: 0.0923 - loss: 6.1484\n",
      "Epoch 47/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 337ms/step - accuracy: 0.0974 - loss: 6.0610\n",
      "Epoch 48/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 323ms/step - accuracy: 0.0992 - loss: 6.1304\n",
      "Epoch 49/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 337ms/step - accuracy: 0.0941 - loss: 6.1528\n",
      "Epoch 50/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 329ms/step - accuracy: 0.0944 - loss: 6.1279\n",
      "Epoch 51/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 322ms/step - accuracy: 0.0961 - loss: 6.1962\n",
      "Epoch 52/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 50ms/step - accuracy: 0.0898 - loss: 6.1160  \n",
      "Epoch 53/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 334ms/step - accuracy: 0.1034 - loss: 5.9991\n",
      "Epoch 54/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 325ms/step - accuracy: 0.1030 - loss: 5.9622\n",
      "Epoch 55/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 323ms/step - accuracy: 0.1013 - loss: 6.0405\n",
      "Epoch 56/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 304ms/step - accuracy: 0.1017 - loss: 5.9873\n",
      "Epoch 57/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 326ms/step - accuracy: 0.0977 - loss: 6.0537\n",
      "Epoch 58/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 307ms/step - accuracy: 0.1021 - loss: 6.0094\n",
      "Epoch 59/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 316ms/step - accuracy: 0.1021 - loss: 5.9907\n",
      "Epoch 60/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 320ms/step - accuracy: 0.1033 - loss: 6.0186\n",
      "Epoch 61/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 323ms/step - accuracy: 0.1013 - loss: 6.0167\n",
      "Epoch 62/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 306ms/step - accuracy: 0.1015 - loss: 6.0339\n",
      "Epoch 63/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 294ms/step - accuracy: 0.1035 - loss: 6.0783\n",
      "Epoch 64/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 291ms/step - accuracy: 0.1030 - loss: 6.0160\n",
      "Epoch 65/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 44ms/step - accuracy: 0.0940 - loss: 6.0394  \n",
      "Epoch 66/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 340ms/step - accuracy: 0.1014 - loss: 5.9248\n",
      "Epoch 67/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 326ms/step - accuracy: 0.1100 - loss: 5.8947\n",
      "Epoch 68/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 311ms/step - accuracy: 0.1048 - loss: 5.9449\n",
      "Epoch 69/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 318ms/step - accuracy: 0.0995 - loss: 5.9657\n",
      "Epoch 70/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 328ms/step - accuracy: 0.1058 - loss: 5.8923\n",
      "Epoch 71/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 298ms/step - accuracy: 0.1074 - loss: 5.9597\n",
      "Epoch 72/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 287ms/step - accuracy: 0.0979 - loss: 6.0109\n",
      "Epoch 73/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 327ms/step - accuracy: 0.1080 - loss: 5.9523\n",
      "Epoch 74/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 303ms/step - accuracy: 0.0989 - loss: 6.0273\n",
      "Epoch 75/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 182ms/step - accuracy: 0.1070 - loss: 5.9714\n",
      "Epoch 76/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 145ms/step - accuracy: 0.1030 - loss: 6.0168\n",
      "Epoch 77/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 138ms/step - accuracy: 0.1092 - loss: 6.0035\n",
      "Epoch 78/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.1034 - loss: 5.9019  \n",
      "Epoch 79/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 142ms/step - accuracy: 0.1061 - loss: 5.8708\n",
      "Epoch 80/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 140ms/step - accuracy: 0.1167 - loss: 5.8369\n",
      "Epoch 81/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 139ms/step - accuracy: 0.1140 - loss: 5.8414\n",
      "Epoch 82/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 142ms/step - accuracy: 0.1128 - loss: 5.8760\n",
      "Epoch 83/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 142ms/step - accuracy: 0.1160 - loss: 5.8570\n",
      "Epoch 84/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 142ms/step - accuracy: 0.1143 - loss: 5.8586\n",
      "Epoch 85/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 145ms/step - accuracy: 0.1190 - loss: 5.8889\n",
      "Epoch 86/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 148ms/step - accuracy: 0.1071 - loss: 5.8649\n",
      "Epoch 87/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 147ms/step - accuracy: 0.1152 - loss: 5.8662\n",
      "Epoch 88/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 149ms/step - accuracy: 0.1111 - loss: 5.9022\n",
      "Epoch 89/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 155ms/step - accuracy: 0.1083 - loss: 5.8921\n",
      "Epoch 90/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 152ms/step - accuracy: 0.1064 - loss: 5.9323\n",
      "Epoch 91/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.1040 - loss: 5.9359  \n",
      "Epoch 92/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 149ms/step - accuracy: 0.1131 - loss: 5.7296\n",
      "Epoch 93/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 149ms/step - accuracy: 0.1189 - loss: 5.7574\n",
      "Epoch 94/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 149ms/step - accuracy: 0.1110 - loss: 5.8287\n",
      "Epoch 95/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 151ms/step - accuracy: 0.1075 - loss: 5.8531\n",
      "Epoch 96/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 151ms/step - accuracy: 0.1215 - loss: 5.7709\n",
      "Epoch 97/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 165ms/step - accuracy: 0.1180 - loss: 5.8194\n",
      "Epoch 98/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 201ms/step - accuracy: 0.1106 - loss: 5.8223\n",
      "Epoch 99/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 203ms/step - accuracy: 0.1146 - loss: 5.8250\n",
      "Epoch 100/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 198ms/step - accuracy: 0.1072 - loss: 5.8721\n",
      "Epoch 101/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 202ms/step - accuracy: 0.1125 - loss: 5.8750\n",
      "Epoch 102/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 209ms/step - accuracy: 0.1137 - loss: 5.8522\n",
      "Epoch 103/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 202ms/step - accuracy: 0.1151 - loss: 5.8694\n",
      "Epoch 104/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 31ms/step - accuracy: 0.1208 - loss: 5.7745  \n",
      "Epoch 105/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 206ms/step - accuracy: 0.1237 - loss: 5.6870\n",
      "Epoch 106/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 208ms/step - accuracy: 0.1138 - loss: 5.7513\n",
      "Epoch 107/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 204ms/step - accuracy: 0.1199 - loss: 5.7304\n",
      "Epoch 108/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 206ms/step - accuracy: 0.1218 - loss: 5.6892\n",
      "Epoch 109/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 206ms/step - accuracy: 0.1115 - loss: 5.7467\n",
      "Epoch 110/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 205ms/step - accuracy: 0.1216 - loss: 5.7634\n",
      "Epoch 111/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 208ms/step - accuracy: 0.1162 - loss: 5.8162\n",
      "Epoch 112/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 201ms/step - accuracy: 0.1069 - loss: 5.8260\n",
      "Epoch 113/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 197ms/step - accuracy: 0.1174 - loss: 5.7508\n",
      "Epoch 114/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 202ms/step - accuracy: 0.1155 - loss: 5.8425\n",
      "Epoch 115/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 196ms/step - accuracy: 0.1175 - loss: 5.8252\n",
      "Epoch 116/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 193ms/step - accuracy: 0.1129 - loss: 5.8418\n",
      "Epoch 117/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.1099 - loss: 5.8106  \n",
      "Epoch 118/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 193ms/step - accuracy: 0.1176 - loss: 5.6728\n",
      "Epoch 119/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 196ms/step - accuracy: 0.1132 - loss: 5.7133\n",
      "Epoch 120/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 205ms/step - accuracy: 0.1161 - loss: 5.7444\n",
      "Epoch 121/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.1172 - loss: 5.7312\n",
      "Epoch 122/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 186ms/step - accuracy: 0.1193 - loss: 5.7173\n",
      "Epoch 123/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 203ms/step - accuracy: 0.1203 - loss: 5.6853\n",
      "Epoch 124/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.1210 - loss: 5.6728\n",
      "Epoch 125/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 196ms/step - accuracy: 0.1113 - loss: 5.7351\n",
      "Epoch 126/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 197ms/step - accuracy: 0.1245 - loss: 5.7162\n",
      "Epoch 127/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 185ms/step - accuracy: 0.1146 - loss: 5.7516\n",
      "Epoch 128/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 187ms/step - accuracy: 0.1141 - loss: 5.7781\n",
      "Epoch 129/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 180ms/step - accuracy: 0.1181 - loss: 5.8061\n",
      "Epoch 130/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.1062 - loss: 5.7339  \n",
      "Epoch 131/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 192ms/step - accuracy: 0.1190 - loss: 5.6178\n",
      "Epoch 132/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 179ms/step - accuracy: 0.1136 - loss: 5.6992\n",
      "Epoch 133/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 188ms/step - accuracy: 0.1179 - loss: 5.7076\n",
      "Epoch 134/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 184ms/step - accuracy: 0.1192 - loss: 5.6820\n",
      "Epoch 135/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 195ms/step - accuracy: 0.1213 - loss: 5.6380\n",
      "Epoch 136/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 187ms/step - accuracy: 0.1204 - loss: 5.6643\n",
      "Epoch 137/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step - accuracy: 0.1172 - loss: 5.7108\n",
      "Epoch 138/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 187ms/step - accuracy: 0.1085 - loss: 5.7259\n",
      "Epoch 139/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step - accuracy: 0.1216 - loss: 5.6409\n",
      "Epoch 140/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.1236 - loss: 5.7133\n",
      "Epoch 141/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 175ms/step - accuracy: 0.1162 - loss: 5.7094\n",
      "Epoch 142/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 172ms/step - accuracy: 0.1181 - loss: 5.7444\n",
      "Epoch 143/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.1221 - loss: 5.7180  \n",
      "Epoch 144/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.1126 - loss: 5.6116\n",
      "Epoch 145/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 170ms/step - accuracy: 0.1201 - loss: 5.6248\n",
      "Epoch 146/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 195ms/step - accuracy: 0.1303 - loss: 5.5423\n",
      "Epoch 147/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 191ms/step - accuracy: 0.1225 - loss: 5.5800\n",
      "Epoch 148/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 189ms/step - accuracy: 0.1177 - loss: 5.6717\n",
      "Epoch 149/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 197ms/step - accuracy: 0.1258 - loss: 5.6387\n",
      "Epoch 150/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 184ms/step - accuracy: 0.1200 - loss: 5.7076\n",
      "Epoch 151/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 187ms/step - accuracy: 0.1276 - loss: 5.6220\n",
      "Epoch 152/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 184ms/step - accuracy: 0.1287 - loss: 5.6557\n",
      "Epoch 153/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 179ms/step - accuracy: 0.1248 - loss: 5.6672\n",
      "Epoch 154/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 179ms/step - accuracy: 0.1175 - loss: 5.6968\n",
      "Epoch 155/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 185ms/step - accuracy: 0.1157 - loss: 5.7351\n",
      "Epoch 156/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.1206 - loss: 5.6228  \n",
      "Epoch 157/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.1226 - loss: 5.6038\n",
      "Epoch 158/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 182ms/step - accuracy: 0.1208 - loss: 5.6348\n",
      "Epoch 159/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 179ms/step - accuracy: 0.1264 - loss: 5.6151\n",
      "Epoch 160/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 173ms/step - accuracy: 0.1275 - loss: 5.5616\n",
      "Epoch 161/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 181ms/step - accuracy: 0.1151 - loss: 5.6310\n",
      "Epoch 162/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 178ms/step - accuracy: 0.1243 - loss: 5.6540\n",
      "Epoch 163/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 183ms/step - accuracy: 0.1202 - loss: 5.6914\n",
      "Epoch 164/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 188ms/step - accuracy: 0.1216 - loss: 5.7038\n",
      "Epoch 165/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 183ms/step - accuracy: 0.1211 - loss: 5.6884\n",
      "Epoch 166/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 188ms/step - accuracy: 0.1168 - loss: 5.6808\n",
      "Epoch 167/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 178ms/step - accuracy: 0.1209 - loss: 5.6717\n",
      "Epoch 168/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 179ms/step - accuracy: 0.1241 - loss: 5.6524\n",
      "Epoch 169/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 27ms/step - accuracy: 0.1223 - loss: 5.6204  \n",
      "Epoch 170/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 186ms/step - accuracy: 0.1223 - loss: 5.5374\n",
      "Epoch 171/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 177ms/step - accuracy: 0.1209 - loss: 5.5776\n",
      "Epoch 172/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 186ms/step - accuracy: 0.1268 - loss: 5.5789\n",
      "Epoch 173/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 176ms/step - accuracy: 0.1237 - loss: 5.5795\n",
      "Epoch 174/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 170ms/step - accuracy: 0.1236 - loss: 5.5830\n",
      "Epoch 175/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 174ms/step - accuracy: 0.1241 - loss: 5.6462\n",
      "Epoch 176/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 171ms/step - accuracy: 0.1256 - loss: 5.6088\n",
      "Epoch 177/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 182ms/step - accuracy: 0.1330 - loss: 5.5925\n",
      "Epoch 178/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 185ms/step - accuracy: 0.1235 - loss: 5.6591\n",
      "Epoch 179/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 179ms/step - accuracy: 0.1193 - loss: 5.6650\n",
      "Epoch 180/180\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 186ms/step - accuracy: 0.1231 - loss: 5.7159\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22ed488fcb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.layers import LayerNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"Roman-Urdu-Poetry.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\n', ' ', text)  # Remove newline characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing and drop NaN values\n",
    "df = df.dropna(subset=[\"Poetry\"])\n",
    "df[\"Cleaned_Poetry\"] = df[\"Poetry\"].apply(preprocess_text)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df[\"Cleaned_Poetry\"])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(df[\"Cleaned_Poetry\"])\n",
    "\n",
    "# Create input-output sequences\n",
    "input_sequences = []\n",
    "output_words = []\n",
    "for seq in sequences:\n",
    "    for i in range(1, len(seq)):  \n",
    "        input_sequences.append(seq[:i])  \n",
    "        output_words.append(seq[i])  \n",
    "\n",
    "# Limit sequence length to avoid memory issues\n",
    "max_seq_length = min(max(len(seq) for seq in input_sequences), 100)  \n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "# Convert y to numpy array and ensure it's within range\n",
    "y = np.array(output_words, dtype=np.int32)\n",
    "\n",
    "# Fix shape issue\n",
    "y = y.reshape(-1, 1)  # Ensure it's 2D\n",
    "\n",
    "# Debugging prints\n",
    "print(f\"Training Data Shape: X={X.shape}, y={y.shape}\")\n",
    "print(f\"Min y value: {y.min()}, Max y value: {y.max()}, Vocab Size: {vocab_size}\")\n",
    "\n",
    "# Define and compile the LSTM model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=100, input_length=max_seq_length),\n",
    "    LSTM(128, return_sequences=False),\n",
    "    LayerNormalization(),  \n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=SparseCategoricalCrossentropy(), optimizer=Adam(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=180,steps_per_epoch=100, batch_size=128, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b42cf9a-ed35-44a1-9b47-344ca7ae2e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "305db5e5-3d73-4f01-ae1f-fdf6102cf4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Poetry: mohabbat ha dil  me sharb th fus y hai har goy gul hai kyuu nah milt hud ki asad mahrib me phuul maan karo is qadar sharminda nah khult sab shahr k chashmemast me tere shnejun ke lahje se th bosaelab aas ke ki rifqat\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def top_p_sampling(probabilities, p=0.9):\n",
    "    \"\"\"\n",
    "    Nucleus Sampling (Top-p Sampling): Selects words from the smallest set whose cumulative probability mass is >= p.\n",
    "    \"\"\"\n",
    "    sorted_indices = np.argsort(probabilities)[::-1]  # Sort words by probability (descending)\n",
    "    sorted_probs = probabilities[sorted_indices]\n",
    "    cumulative_probs = np.cumsum(sorted_probs)  # Compute cumulative probabilities\n",
    "\n",
    "    # Get smallest set of words covering at least p probability mass\n",
    "    selected_indices = sorted_indices[cumulative_probs <= p]\n",
    "    if len(selected_indices) == 0:\n",
    "        selected_indices = sorted_indices[:1]  # Always include at least one word\n",
    "\n",
    "    # Sample a word from selected_indices\n",
    "    chosen_index = np.random.choice(selected_indices)\n",
    "    return chosen_index\n",
    "\n",
    "def generate_poetry(seed_text, next_words=10, temperature=0.7, top_p=0.9):\n",
    "    generated_words = []  # Track last 3 generated words to prevent repetition\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "        # Predict next word probabilities\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "\n",
    "        # Apply temperature scaling\n",
    "        predicted_probs = np.exp(np.log(predicted_probs + 1e-8) / temperature)\n",
    "        predicted_probs /= np.sum(predicted_probs)  # Normalize probabilities\n",
    "\n",
    "        # Select word using Nucleus Sampling (Top-p)\n",
    "        selected_index = top_p_sampling(predicted_probs, top_p)\n",
    "\n",
    "        # Convert index to word\n",
    "        output_word = tokenizer.index_word.get(selected_index, None)\n",
    "\n",
    "        # Ensure valid word and avoid repetition\n",
    "        if not output_word or selected_index == 0 or output_word in generated_words[-3:]:\n",
    "            continue  # Skip unknown or repeated words\n",
    "\n",
    "        # Append predicted word to the seed text and track it\n",
    "        seed_text += \" \" + output_word\n",
    "        generated_words.append(output_word)\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "# Example usage\n",
    "seed = \"mohabbat ha dil \"\n",
    "generated_poetry = generate_poetry(seed_text=seed, next_words=200, temperature=0.7, top_p=0.9)\n",
    "print(\"Generated Poetry:\", generated_poetry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "add10a30-1958-4b89-b4aa-dbd8a35a61b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.save(\"poetry_model.h5\")\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(tokenizer, handle)\n",
    "with open(\"max_seq_length.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(max_seq_length, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21fabf1e-76a9-4a7b-8c39-097f55874353",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "2025-02-17 11:13:48.672 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.673 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.674 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.676 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.677 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.678 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.679 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.680 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.681 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.682 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.683 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.685 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.686 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.687 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.688 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.689 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.690 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.691 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.692 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.693 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.694 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-02-17 11:13:48.696 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"poetry_model.h5\")\n",
    "\n",
    "# Load tokenizer\n",
    "with open(\"tokenizer.pkl\", \"rb\") as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "\n",
    "# Load max sequence length\n",
    "with open(\"max_seq_length.pkl\", \"rb\") as handle:\n",
    "    max_seq_length = pickle.load(handle)\n",
    "\n",
    "# Poetry generation function\n",
    "def generate_poetry(seed_text, next_words=10, temperature=1.0):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_seq_length, padding='pre')\n",
    "\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        predicted_probs = np.log(predicted_probs + 1e-8) / temperature\n",
    "        exp_preds = np.exp(predicted_probs)\n",
    "        predicted_probs = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
    "        output_word = tokenizer.index_word.get(predicted_index, None)\n",
    "\n",
    "        if not output_word or predicted_index == 0:\n",
    "            continue\n",
    "\n",
    "        seed_text += \" \" + output_word\n",
    "\n",
    "    return seed_text\n",
    "\n",
    "# Streamlit UI\n",
    "st.title(\"Roman Urdu Poetry Generator 🎤✨\")\n",
    "st.write(\"Enter a phrase, and the AI will generate poetry in Roman Urdu.\")\n",
    "\n",
    "# User input for starting phrase\n",
    "user_input = st.text_input(\"Enter a starting phrase:\", \"\")\n",
    "\n",
    "# User input for number of words\n",
    "num_words = st.number_input(\"Enter the number of words to generate:\", min_value=1, max_value=100, value=10)\n",
    "\n",
    "if st.button(\"Generate Poetry\"):\n",
    "    if user_input.strip():\n",
    "        poetry = generate_poetry(user_input, next_words=num_words, temperature=0.7)\n",
    "        st.subheader(\"Generated Poetry:\")\n",
    "        st.write(poetry)\n",
    "    else:\n",
    "        st.warning(\"Please enter a valid phrase to start!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cb4c4a-35fe-4b40-a1fc-b20b7f464117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
